# main.py
import os, io, zipfile, re, json
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt

# ì§€ë„ìš©
import geopandas as gpd
from shapely.geometry import shape

st.set_page_config(page_title="ì„œìš¸ì‹œ ìƒê¶Œ ë¶„ê¸° ë§¤ì¶œ ì§€ë„ ë¶„ì„", layout="wide")

# ---- ê³ ì •: ì›ë³¸ ì—´ì´ë¦„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í•´ì„ì€ 'ë¶„ê¸°')
QUARTER_COL = "ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ"
AMT_COL     = "ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡"   # ë¶„ê¸°ê¸ˆì•¡ìœ¼ë¡œ í•´ì„í•˜ì—¬ í‘œì‹œ
CNT_COL     = "ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜"

MILLION = 1_000_000  # ë°±ë§Œì› í™˜ì‚°

# ---- ìœ í‹¸
def to_num(s):
    return pd.to_numeric(pd.Series(s, dtype="object").astype(str).str.replace(",", "", regex=False), errors="coerce")

def parse_yq(s: str):
    s = str(s)
    m = re.search(r"(20\d{2}).*?([1-4])", s)
    if m: return int(m.group(1)), int(m.group(2))
    if s.isdigit() and len(s) in (5,6): return int(s[:4]), int(s[-1])
    return (9999, 9)

def pct(a, b):
    return (a/b - 1) * 100 if (b and b != 0) else np.nan

def read_csv_any(file):
    for enc in ["utf-8-sig","cp949","euc-kr"]:
        try:
            df = pd.read_csv(file, encoding=enc)
            df.columns = df.columns.map(lambda c: str(c).strip().replace("\u200b",""))
            return df
        except Exception:
            file.seek(0) if hasattr(file, "seek") else None
            continue
    return None

def read_xls_header2(file_or_path):
    df = pd.read_excel(file_or_path, sheet_name=0, header=2)
    df.columns = df.columns.map(lambda c: str(c).strip().replace("\u200b",""))
    return df

def load_geo_from_zip_or_geojson(uploaded):
    # uploaded: BytesIO/UploadedFile
    name = uploaded.name.lower()
    if name.endswith(".geojson") or name.endswith(".json"):
        gj = json.load(uploaded)
        gdf = gpd.GeoDataFrame.from_features(gj["features"])
        return gdf
    elif name.endswith(".zip"):
        # shapefile zip
        mem = io.BytesIO(uploaded.read())
        with zipfile.ZipFile(mem) as zf:
            tmpdir = "./_shp_tmp"
            os.makedirs(tmpdir, exist_ok=True)
            zf.extractall(tmpdir)
            # find .shp
            shp_paths = [os.path.join(tmpdir, f) for f in os.listdir(tmpdir) if f.lower().endswith(".shp")]
            if not shp_paths:
                raise RuntimeError("zip ì•ˆì— .shp íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            gdf = gpd.read_file(shp_paths[0])
            return gdf
    else:
        raise RuntimeError("ì§€ì›í•˜ëŠ” ì§€ë„ í˜•ì‹ì€ .geojson/.json ë˜ëŠ” .zip(shapefile) ì…ë‹ˆë‹¤.")

# ---- ì—…ë¡œë“œ
st.title("ğŸ—ºï¸ ì„œìš¸ì‹œ ìƒê¶Œ ë¶„ê¸° ë§¤ì¶œ ì§€ë„ ë¶„ì„ (ë‹¨ìœ„: ë°±ë§Œì›)")

c1, c2 = st.columns([1,1])
with c1:
    sales_file = st.file_uploader("â‘  ë§¤ì¶œ CSV ì—…ë¡œë“œ", type=["csv"])
    attr_file  = st.file_uploader("â‘¡ ìƒê¶Œ ì†ì„± XLS ì—…ë¡œë“œ", type=["xls","xlsx"])
with c2:
    geo_file   = st.file_uploader("â‘¢ í–‰ì •ë™ ê²½ê³„ ì—…ë¡œë“œ (.geojson/.json ë˜ëŠ” .zip(shp))", type=["geojson","json","zip"])
    code_file  = st.file_uploader("â‘£ (ì„ íƒ) í–‰ì •êµ¬ì—­ ì½”ë“œí‘œ XLS (ìë©´ë™ ì½”ë“œ/ëª…ì¹­ í‘œì¤€í™”)", type=["xls","xlsx"])

if not (sales_file and attr_file and geo_file):
    st.info("ì„¸ íŒŒì¼(â‘ â‘¡â‘¢)ì„ ëª¨ë‘ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    st.stop()

# ---- ë¡œë”©
sales = read_csv_any(sales_file)
if sales is None or sales.empty:
    st.error("ë§¤ì¶œ CSVë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); st.stop()

attr = read_xls_header2(attr_file)
geo = load_geo_from_zip_or_geojson(geo_file)

# ì»¬ëŸ¼ ì •ë¦¬
for df in (sales, attr, geo):
    df.columns = df.columns.map(lambda c: str(c).strip().replace("\u200b",""))

# í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì‚¬
need_sales = [QUARTER_COL, AMT_COL, CNT_COL]
miss = [c for c in need_sales if c not in sales.columns]
if miss: st.error(f"ë§¤ì¶œ CSV í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {miss}"); st.stop()

need_attr = ["ìƒê¶Œì½”ë“œ","ìƒê¶Œëª…","í–‰ì •ë™","ìì¹˜êµ¬","ë©´ì (ã¡)"]
miss2 = [c for c in need_attr if c not in attr.columns]
if miss2: st.error(f"ìƒê¶Œ ì†ì„± XLS í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {miss2}"); st.stop()

# í–‰ì •ë™ ê²½ê³„ ì†ì„± í›„ë³´ íŒŒì•…
adm_cd_col = None; adm_nm_col = None
for c in geo.columns:
    if c.upper() in ("ADM_CD","EMD_CD","DONG_CD","ë²•ì •ë™ì½”ë“œ","ë²•ì •ë™ì½”ë“œ10","í–‰ì •ë™ì½”ë“œ"): adm_cd_col = c
    if c.upper() in ("ADM_NM","EMD_NM","DONG_NM","ë²•ì •ë™ëª…","í–‰ì •ë™ëª…"):       adm_nm_col = c
if adm_cd_col is None and adm_nm_col is None:
    st.error("ê²½ê³„ íŒŒì¼ì—ì„œ í–‰ì •ë™ ì½”ë“œ/ëª… ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì†ì„± ì—´ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# (ì„ íƒ) ì½”ë“œí‘œ ë¡œë“œ â†’ ìë©´ë™ëª…â‡„ì½”ë“œ ë³´ì •
if code_file:
    code = pd.read_excel(code_file, sheet_name=0, header=1)
    code.columns = ["ì‹œë„ì½”ë“œ","ì‹œë„ëª…ì¹­","ì‹œêµ°êµ¬ì½”ë“œ","ì‹œêµ°êµ¬ëª…ì¹­","ìë©´ë™ì½”ë“œ","ìë©´ë™ëª…ì¹­"]
    code["ìë©´ë™ì½”ë“œ"] = code["ìë©´ë™ì½”ë“œ"].astype(str).str.zfill(10)
else:
    code = None

# ---- ë§¤ì¶œâ†”ìƒê¶Œì†ì„± ë³‘í•© (ìƒê¶Œ ì½”ë“œ/ëª…)
# ë§¤ì¶œë°ì´í„° ìª½ ìƒê¶Œ í‚¤ í›„ë³´: ìƒê¶Œ_ì½”ë“œ ë˜ëŠ” ìƒê¶Œ_ì½”ë“œ_ëª… / ìƒê¶Œ_ì½”ë“œ_ëª…(ì´ë¦„)
sales_keys = [c for c in sales.columns if "ìƒê¶Œ" in c and ("ì½”ë“œ" in c or "ëª…" in c)]
attr_keys  = ["ìƒê¶Œì½”ë“œ","ìƒê¶Œëª…"]

# ìš°ì„  ì½”ë“œë¡œ, ì—†ìœ¼ë©´ ëª…ì¹­ìœ¼ë¡œ ë³‘í•©
merge_on_sales = None; merge_on_attr = None
if any("ìƒê¶Œ_ì½”ë“œ" == c for c in sales.columns):
    merge_on_sales = "ìƒê¶Œ_ì½”ë“œ"; merge_on_attr = "ìƒê¶Œì½”ë“œ"
elif any("ìƒê¶Œ_ì½”ë“œ" in c for c in sales.columns):
    # ê°€ì¥ ê·¼ì ‘í•œ ì—´ í•˜ë‚˜
    merge_on_sales = [c for c in sales.columns if "ìƒê¶Œ_ì½”ë“œ" in c][0]; merge_on_attr="ìƒê¶Œì½”ë“œ"
elif any("ìƒê¶Œ_ì½”ë“œ_ëª…" == c for c in sales.columns):
    merge_on_sales = "ìƒê¶Œ_ì½”ë“œ_ëª…"; merge_on_attr = "ìƒê¶Œëª…"
elif any("ìƒê¶Œ_ì½”ë“œ_ëª…" in c for c in sales.columns):
    merge_on_sales = [c for c in sales.columns if "ìƒê¶Œ_ì½”ë“œ_ëª…" in c][0]; merge_on_attr="ìƒê¶Œëª…"
else:
    st.error(f"ë§¤ì¶œ CSVì—ì„œ ìƒê¶Œ í‚¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í›„ë³´ ì—´: {sales_keys}")
    st.stop()

merged = sales.merge(attr, left_on=merge_on_sales, right_on=merge_on_attr, how="left")

# ---- í–‰ì •ë™ ë§¤í•‘ (í–‰ì •ë™ëª… â†’ ì½”ë“œ, ë˜ëŠ” ì½”ë“œ ì§ì ‘)
if "í–‰ì •ë™" not in merged.columns and "í–‰ì •ë™ëª…" in merged.columns:
    merged = merged.rename(columns={"í–‰ì •ë™ëª…":"í–‰ì •ë™"})

if code is not None and ("ìë©´ë™ì½”ë“œ" in code.columns):
    # í–‰ì •ë™ëª… ê¸°ì¤€ìœ¼ë¡œ ì½”ë“œ ë§¤í•‘
    merged = merged.merge(code[["ìë©´ë™ì½”ë“œ","ìë©´ë™ëª…ì¹­"]],
                          left_on="í–‰ì •ë™", right_on="ìë©´ë™ëª…ì¹­", how="left")
    merged["í–‰ì •ë™ì½”ë“œ"] = merged["ìë©´ë™ì½”ë“œ"].astype(str).str.zfill(10)
else:
    # ì½”ë“œí‘œ ì—†ìœ¼ë©´ ëª…ì¹­ìœ¼ë¡œë§Œ ì¡°ì¸í•  ì¤€ë¹„
    merged["í–‰ì •ë™ì½”ë“œ"] = np.nan

# ---- ì‚¬ì´ë“œë°” í•„í„°
with st.sidebar:
    st.header("ğŸ” í•„í„°")
    uniq_q = sorted(merged[QUARTER_COL].astype(str).unique(), key=parse_yq)
    default_last = uniq_q[-5:] if len(uniq_q)>=5 else uniq_q
    picked = st.multiselect("ë¶„ê¸° ì„ íƒ", uniq_q, default_last)

    # ìì¹˜êµ¬/ìƒê¶Œë¶„ë¥˜ í•„í„° (ìˆì„ ë•Œ)
    chosen = []
    for col in ["ìì¹˜êµ¬","ìƒê¶Œë¶„ë¥˜"]:
        if col in merged.columns:
            vals = sorted(merged[col].dropna().astype(str).unique())
            sel = st.multiselect(f"{col} ì„ íƒ", vals)
            if sel: chosen.append((col, sel))

    st.divider()
    TOPN = st.slider("Top-N", 5, 50, 20, 1)

work = merged.copy()
if picked:
    work = work[work[QUARTER_COL].astype(str).isin(picked)]
for c,vals in chosen:
    work = work[work[c].astype(str).isin(vals)]
if work.empty:
    st.warning("í•„í„° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

# ---- KPI (ë°±ë§Œì›)
st.subheader("ğŸ“Œ ìš”ì•½ (ë‹¨ìœ„: ë°±ë§Œì›)")
sum_amt_m = to_num(work[AMT_COL]).sum() / MILLION
sum_cnt   = to_num(work[CNT_COL]).sum()
avg_price_m = (sum_amt_m / sum_cnt) if sum_cnt else np.nan
c1,c2,c3 = st.columns(3)
c1.metric("ì´ ë§¤ì¶œ(ë°±ë§Œì›)", f"{sum_amt_m:,.1f}")
c2.metric("ì´ ê±´ìˆ˜", f"{sum_cnt:,.0f} ê±´")
c3.metric("í‰ê·  ê°ë‹¨ê°€(ë°±ë§Œì›/ê±´)", f"{avg_price_m:,.3f}" if pd.notna(avg_price_m) else "N/A")

# ---- ë¶„ê¸°ë³„ ë§¤ì¶œ ì¶”ì´ (ë§‰ëŒ€, ë°±ë§Œì›)
st.subheader("ğŸ“ˆ ë¶„ê¸°ë³„ ë§¤ì¶œ ì¶”ì´ (ë§‰ëŒ€, ë‹¨ìœ„: ë°±ë§Œì›)")
trend = (
    work.groupby(QUARTER_COL)[AMT_COL]
        .agg(lambda s: to_num(s).sum())
        .reset_index(name="ë§¤ì¶œ ê¸ˆì•¡(ë°±ë§Œì›)")
        .sort_values(by=QUARTER_COL, key=lambda s: s.astype(str).map(parse_yq))
)
trend["ë§¤ì¶œ ê¸ˆì•¡(ë°±ë§Œì›)"] = trend["ë§¤ì¶œ ê¸ˆì•¡(ë°±ë§Œì›)"] / MILLION
st.altair_chart(
    alt.Chart(trend).mark_bar().encode(
        x=alt.X(f"{QUARTER_COL}:N", title="ë¶„ê¸°"),
        y=alt.Y("ë§¤ì¶œ ê¸ˆì•¡(ë°±ë§Œì›):Q", title="ë§¤ì¶œ ê¸ˆì•¡(ë°±ë§Œì›)"),
        tooltip=[QUARTER_COL, alt.Tooltip("ë§¤ì¶œ ê¸ˆì•¡(ë°±ë§Œì›):Q", format=",.1f")]
    ),
    use_container_width=True
)

# ---- í–‰ì •ë™ ë‹¨ìœ„ ì§‘ê³„ (ì§€ë„ìš©)
# 1) í–‰ì •ë™ì½”ë“œê°€ ìˆìœ¼ë©´ ì½”ë“œ ê¸°ì¤€, ì—†ìœ¼ë©´ ëª…ì¹­ ê¸°ì¤€ìœ¼ë¡œ geoì™€ ì¡°ì¸
geo = geo.copy()
if adm_cd_col:
    # shapefile ì½”ë“œ ë¬¸ìì—´ í†µì¼
    geo[adm_cd_col] = geo[adm_cd_col].astype(str).str.zfill(10)
    if "í–‰ì •ë™ì½”ë“œ" in work.columns and work["í–‰ì •ë™ì½”ë“œ"].notna().any():
        agg = (
            work.groupby("í–‰ì •ë™ì½”ë“œ")[AMT_COL]
                .agg(lambda s: to_num(s).sum())
                .reset_index(name="ë§¤ì¶œ(ë°±ë§Œì›)")
        )
        agg["ë§¤ì¶œ(ë°±ë§Œì›)"] = agg["ë§¤ì¶œ(ë°±ë§Œì›)"] / MILLION
        joined = geo.merge(agg, left_on=adm_cd_col, right_on="í–‰ì •ë™ì½”ë“œ", how="left")
    else:
        # ëª…ì¹­ ê¸°ì¤€
        base_nm = adm_nm_col if adm_nm_col else adm_cd_col
        agg = (
            work.groupby("í–‰ì •ë™")[AMT_COL]
                .agg(lambda s: to_num(s).sum())
                .reset_index(name="ë§¤ì¶œ(ë°±ë§Œì›)")
        )
        agg["ë§¤ì¶œ(ë°±ë§Œì›)"] = agg["ë§¤ì¶œ(ë°±ë§Œì›)"] / MILLION
        if adm_nm_col:
            joined = geo.merge(agg, left_on=adm_nm_col, right_on="í–‰ì •ë™", how="left")
        else:
            st.warning("ê²½ê³„ì— í–‰ì •ë™ëª… ì»¬ëŸ¼ì´ ì—†ì–´ ëª…ì¹­ ê¸°ì¤€ ì¡°ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤. ì½”ë“œí‘œ ì—…ë¡œë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            joined = geo.copy()
else:
    # ì½”ë“œ ì»¬ëŸ¼ì´ ì „í˜€ ì—†ì„ ë•Œ: ëª…ì¹­ ê¸°ì¤€ë§Œ ì‹œë„
    agg = (
        work.groupby("í–‰ì •ë™")[AMT_COL]
            .agg(lambda s: to_num(s).sum())
            .reset_index(name="ë§¤ì¶œ(ë°±ë§Œì›)")
    )
    agg["ë§¤ì¶œ(ë°±ë§Œì›)"] = agg["ë§¤ì¶œ(ë°±ë§Œì›)"] / MILLION
    if adm_nm_col:
        joined = geo.merge(agg, left_on=adm_nm_col, right_on="í–‰ì •ë™", how="left")
    else:
        st.error("ê²½ê³„ì—ì„œ ì½”ë“œ/ëª…ì¹­ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); st.stop()

# ---- ì§€ë„ (pydeck: Choropleth)
st.subheader("ğŸ—ºï¸ í–‰ì •ë™ ë§¤ì¶œ Choropleth (ë‹¨ìœ„: ë°±ë§Œì›)")
# ì¤‘ì‹¬ ì¶”ì •
try:
    center = joined.geometry.unary_union.centroid
    view_state = {"latitude": center.y, "longitude": center.x, "zoom": 10}
except Exception:
    view_state = {"latitude": 37.5665, "longitude": 126.9780, "zoom": 10}

# pydeckì€ GeoJSONì´ í¸í•´ ë³€í™˜
gj = json.loads(joined.to_json())

import pydeck as pdk
layer = pdk.Layer(
    "GeoJsonLayer",
    gj,
    opacity=0.6,
    stroked=True,
    get_fill_color="[min(255, 80 + (properties['ë§¤ì¶œ(ë°±ë§Œì›)'] || 0) * 2), 120, 180]",
    get_line_color=[50, 50, 50],
    line_width_min_pixels=1,
    pickable=True,
)
tooltip = {"html": "<b>{properties.ADM_NM}</b><br/>ë§¤ì¶œ: {properties.ë§¤ì¶œ(ë°±ë§Œì›)} ë°±ë§Œì›", "style": {"color": "white"}}

st.pydeck_chart(pdk.Deck(layers=[layer], initial_view_state=pdk.ViewState(**view_state), tooltip=tooltip))

# ---- ë­í‚¹ (í–‰ì •ë™/ìƒê¶Œ/ìƒê¶Œë¶„ë¥˜ Top-N)
st.subheader(f"ğŸ† Top {TOPN}")
col_opts = []
if "í–‰ì •ë™" in work.columns: col_opts.append("í–‰ì •ë™")
if "ìƒê¶Œëª…" in work.columns: col_opts.append("ìƒê¶Œëª…")
if "ìƒê¶Œë¶„ë¥˜" in work.columns: col_opts.append("ìƒê¶Œë¶„ë¥˜")

if col_opts:
    grp_col = st.selectbox("ë­í‚¹ ê¸°ì¤€ ì»¬ëŸ¼", col_opts, index=0)
    rk = (
        work.groupby(grp_col)[AMT_COL]
            .agg(lambda s: to_num(s).sum())
            .reset_index(name="ë§¤ì¶œ(ë°±ë§Œì›)")
            .sort_values("ë§¤ì¶œ(ë°±ë§Œì›)", ascending=False)
            .head(TOPN)
    )
    rk["ë§¤ì¶œ(ë°±ë§Œì›)"] = rk["ë§¤ì¶œ(ë°±ë§Œì›)"] / MILLION
    c1, c2 = st.columns(2)
    with c1:
        st.dataframe(rk, use_container_width=True)
    with c2:
        st.altair_chart(
            alt.Chart(rk).mark_bar().encode(
                x=alt.X("ë§¤ì¶œ(ë°±ë§Œì›):Q", title="ë§¤ì¶œ(ë°±ë§Œì›)"),
                y=alt.Y(f"{grp_col}:N", sort="-x"),
                tooltip=[grp_col, alt.Tooltip("ë§¤ì¶œ(ë°±ë§Œì›):Q", format=",.1f")]
            ), use_container_width=True
        )
else:
    st.info("ë­í‚¹ì„ ê³„ì‚°í•  ì ì ˆí•œ ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ---- ë‹¤ìš´ë¡œë“œ
st.subheader("ğŸ“¥ ì§€ë„ ì¡°ì¸ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
st.download_button(
    "í–‰ì •ë™ ì¡°ì¸ GeoJSON ë‹¤ìš´ë¡œë“œ",
    data=json.dumps(gj, ensure_ascii=False),
    file_name="joined_admdong_sales.geojson",
    mime="application/geo+json"
)
